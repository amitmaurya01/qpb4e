{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaC5qZlZHZ8kRi1lyCDrzl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nceder/qpb4e/blob/main/code/Chapter%2019/scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKER3H4ImKzw",
        "outputId": "e21701df-e344-4f46-c8ee-c240e211f05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/nceder/qpb4e/main/code/Chapter%2006/moby_01.txt &> null  && echo Downloaded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generated by colaboratory\n",
        "# prompt: Package the functions in the cell above as a standalone module so that\n",
        "#         the module can be used by other code. Then write the code to use the\n",
        "#         module to achieve the same result as the cell above.\n",
        "\n",
        "# Save the functions as a module named \"text_processing.py\"\n",
        "with open(\"text_processing.py\", \"w\") as f:\n",
        "    f.write(r\"\"\"\n",
        "def clean_line(line):\n",
        "    # Convert the line to lowercase\n",
        "    line = line.lower()\n",
        "\n",
        "    # Remove punctuation from the line\n",
        "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    return line\n",
        "\n",
        "def write_words_to_file(words, output_file):\n",
        "    # Write each word to the output file\n",
        "    for word in words:\n",
        "        output_file.write(word + '\\n')\n",
        "\n",
        "def count_word_occurrences(words):\n",
        "    # Count the occurrences of each word using a dictionary\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "def print_common_words(word_counts, n):\n",
        "    # Sort the word counts in descending order\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the n most common words and their occurrences\n",
        "    print(f\"{n} most common words:\")\n",
        "    for word, count in sorted_word_counts[:n]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "def print_least_common_words(word_counts, n):\n",
        "    # Sort the word counts in descending order\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the n least common words and their occurrences\n",
        "    print(f\"{n} least common words:\")\n",
        "    for word, count in sorted_word_counts[-n:]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "def process_file(input_file_path, output_file_path):\n",
        "    # Open the input file for reading\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        # Open the output file for writing\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            # Iterate over each line in the input file\n",
        "            for line in input_file:\n",
        "                # Clean the line\n",
        "                cleaned_line = clean_line(line)\n",
        "\n",
        "                # Split the line into words\n",
        "                words = cleaned_line.split()\n",
        "\n",
        "                # Write the words to the output file\n",
        "                write_words_to_file(words, output_file)\n",
        "\"\"\")\n",
        "\n",
        "# Use the module\n",
        "import text_processing\n",
        "\n",
        "input_file_path = 'moby_01.txt'\n",
        "output_file_path = 'moby_01_clean.txt'\n",
        "\n",
        "# Process the file\n",
        "text_processing.process_file(input_file_path, output_file_path)\n",
        "\n",
        "# Open the file\n",
        "with open(output_file_path, 'r') as file:\n",
        "    # Read the file content\n",
        "    content = file.read()\n",
        "\n",
        "# Split the content into words\n",
        "words = content.split()\n",
        "\n",
        "# Count word occurrences\n",
        "word_counts = text_processing.count_word_occurrences(words)\n",
        "\n",
        "# Print the five most common words and their occurrences\n",
        "text_processing.print_common_words(word_counts, 5)\n",
        "\n",
        "# Print the five least common words and their occurrences\n",
        "text_processing.print_least_common_words(word_counts, 5)"
      ],
      "metadata": {
        "id": "5lyqqI4umMiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Refactor the code in module created in cell above into a package containing at least one module for the cleaning functions, and another for the processing functions, and (optionally) one for custom exceptions (if you have any). Then refactor main() to use the new package.\n",
        "\n",
        "# Create a directory for the package\n",
        "!mkdir text_processor\n",
        "\n",
        "# Create a file for the cleaning functions\n",
        "%%writefile text_processor/cleaning.py\n",
        "def clean_line(line):\n",
        "    # Convert the line to lowercase\n",
        "    line = line.lower()\n",
        "\n",
        "    # Remove punctuation from the line\n",
        "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    return line\n",
        "\n",
        "# Create a file for the processing functions\n",
        "%%writefile text_processor/processing.py\n",
        "def write_words_to_file(words, output_file):\n",
        "    # Write each word to the output file\n",
        "    for word in words:\n",
        "        output_file.write(word + '\\n')\n",
        "\n",
        "def count_word_occurrences(words):\n",
        "    # Count the occurrences of each word using a dictionary\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "def print_common_words(word_counts, n):\n",
        "    # Sort the word counts in descending order\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the n most common words and their occurrences\n",
        "    print(f\"{n} most common words:\")\n",
        "    for word, count in sorted_word_counts[:n]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "def print_least_common_words(word_counts, n):\n",
        "    # Sort the word counts in descending order\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the n least common words and their occurrences\n",
        "    print(f\"{n} least common words:\")\n",
        "    for word, count in sorted_word_counts[-n:]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "def process_file(input_file_path, output_file_path):\n",
        "    # Open the input file for reading\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        # Open the output file for writing\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            # Iterate over each line in the input file\n",
        "            for line in input_file:\n",
        "                # Clean the line\n",
        "                cleaned_line = clean_line(line)\n",
        "\n",
        "                # Split the line into words\n",
        "                words = cleaned_line.split()\n",
        "\n",
        "                # Write the words to the output file\n",
        "                write_words_to_file(words, output_file)\n",
        "\n",
        "# Create an __init__.py file to make the directory a package\n",
        "%%writefile text_processor/__init__.py\n",
        "from .cleaning import clean_line\n",
        "from .processing import process_file, count_word_occurrences, print_common_words, print_least_common_words\n",
        "\n",
        "# Refactor main() to use the new package\n",
        "%%writefile main.py\n",
        "import text_processor\n",
        "\n",
        "input_file_path = 'moby_01.txt'\n",
        "output_file_path = 'moby_01_clean.txt'\n",
        "\n",
        "# Process the file\n",
        "text_processor.process_file(input_file_path, output_file_path)\n",
        "\n",
        "# Open the file\n",
        "with open(output_file_path, 'r') as file:\n",
        "    # Read the file content\n",
        "    content = file.read()\n",
        "\n",
        "# Split the content into words\n",
        "words = content.split()\n",
        "\n",
        "# Count word occurrences\n",
        "word_counts = text_processor.count_word_occurrences(words)\n",
        "\n",
        "# Print the five most common words and their occurrences\n",
        "text_processor.print_common_words(word_counts, 5)\n",
        "\n",
        "# Print the five least common words and their occurrences\n",
        "text_processor.print_least_common_words(word_counts, 5)\n",
        "\n",
        "# Run the main script\n",
        "!python main.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbFMa7atmUHW",
        "outputId": "8f93d5ce-7f67-417c-ba12-f0e9a7cec919"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%%writefile` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dvx3-TWxqX17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Refactor the code in module created in cell above into a package containing at least one module for the cleaning functions, and another for the processing functions, and (optionally) one for custom exceptions (if you have any). Then refactor main() to use the new package.\n",
        "\n",
        "# Create a directory for the package\n",
        "!mkdir text_processor\n"
      ],
      "metadata": {
        "id": "dZmZMTkVp363"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a file for the cleaning functions\n",
        "%%writefile text_processor/cleaning.py\n",
        "def clean_line(line):\n",
        "    # Convert the line to lowercase\n",
        "    line = line.lower()\n",
        "\n",
        "    # Remove punctuation from the line\n",
        "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    return line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtkzCFUbnr4T",
        "outputId": "e0d9465e-de32-40e1-8e85-4e031f94812a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing text_processor/cleaning.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a file for the processing functions\n",
        "%%writefile text_processor/processing.py\n",
        "def write_words_to_file(words, output_file):\n",
        "    # Write each word to the output file\n",
        "    for word in words:\n",
        "        output_file.write(word + '\\n')\n",
        "\n",
        "def count_word_occurrences(words):\n",
        "    # Count the occurrences of each word using a dictionary\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "def print_common_words(word_counts, n):\n",
        "    # Sort the word counts in descending order\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the n most common words and their occurrences\n",
        "    print(f\"{n} most common words:\")\n",
        "    for word, count in sorted_word_counts[:n]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "def print_least_common_words(word_counts, n):\n",
        "    # Sort the word counts in descending order\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the n least common words and their occurrences\n",
        "    print(f\"{n} least common words:\")\n",
        "    for word, count in sorted_word_counts[-n:]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "def process_file(input_file_path, output_file_path):\n",
        "    # Open the input file for reading\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        # Open the output file for writing\n",
        "        with open(output_file_path, 'w') as output_file:\n",
        "            # Iterate over each line in the input file\n",
        "            for line in input_file:\n",
        "                # Clean the line\n",
        "                cleaned_line = clean_line(line)\n",
        "\n",
        "                # Split the line into words\n",
        "                words = cleaned_line.split()\n",
        "\n",
        "                # Write the words to the output file\n",
        "                write_words_to_file(words, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBeuPmMWoR7k",
        "outputId": "0eb0df01-cca3-4b37-ad6d-790908ac0332"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing text_processor/processing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an __init__.py file to make the directory a package\n",
        "%%writefile text_processor/__init__.py\n",
        "from .cleaning import clean_line\n",
        "from .processing import process_file, count_word_occurrences, print_common_words, print_least_common_words\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo-9nDn3qAVu",
        "outputId": "04b36773-cf0e-4d13-f7d6-1f87de4a99aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing text_processor/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refactor main() to use the new package\n",
        "%%writefile main.py\n",
        "import text_processor\n",
        "\n",
        "input_file_path = 'moby_01.txt'\n",
        "output_file_path = 'moby_01_clean.txt'\n",
        "\n",
        "# Process the file\n",
        "text_processor.process_file(input_file_path, output_file_path)\n",
        "\n",
        "# Open the file\n",
        "with open(output_file_path, 'r') as file:\n",
        "    # Read the file content\n",
        "    content = file.read()\n",
        "\n",
        "# Split the content into words\n",
        "words = content.split()\n",
        "\n",
        "# Count word occurrences\n",
        "word_counts = text_processor.count_word_occurrences(words)\n",
        "\n",
        "# Print the five most common words and their occurrences\n",
        "text_processor.print_common_words(word_counts, 5)\n",
        "\n",
        "# Print the five least common words and their occurrences\n",
        "text_processor.print_least_common_words(word_counts, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbPKloCqqFWA",
        "outputId": "5ce2b549-c63b-4105-9802-d7e9f6437fae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the main script\n",
        "!python main.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flbMH0-RqKoM",
        "outputId": "d555e70f-7fba-4cf8-982e-705a2d94230b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/main.py\", line 7, in <module>\n",
            "    text_processor.process_file(input_file_path, output_file_path)\n",
            "  File \"/content/text_processor/processing.py\", line 43, in process_file\n",
            "    cleaned_line = clean_line(line)\n",
            "NameError: name 'clean_line' is not defined. Did you mean: 'cleaned_line'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QrFTfx8uqc30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}